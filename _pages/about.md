---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Iâ€™m now a fourthâ€year undergraduate student in Northeastern University (Shenyang, China), and affiliated with the School of Computer Science and Engineering.

My research interests include Deep Learning(DL), Multimodal Learning, Natural Language Processing(NLP) and Information Retrieval(IR).

I am looking for a gap-year RA/Phd 24-Spring/Phd 24-Fall.

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/UniVL-DR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<!-- [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) -->
Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval

[[pdf](https://openreview.net/pdf?id=PQOlkgsBsik)][[code](https://github.com/OpenMatch/UniVL-DR)]

Zhenghao Liu, Chenyan Xiong, **Yuanhuiyi Lv**, Zhiyuan Liu, Ge Yu

This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR), which builds a unified model for multi-modal retrieval. UniVL-DR encodes queries and multi-modality resources in an embedding space for searching candidates from different modalities.

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

# ğŸ“ Projects

<div class='paper-box'><div class='paper-box-image'><div><img src='images/prompt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<!-- [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) -->
[Prompt Tuning For Sentiment Classification Base On Preâ€trained Language Models](https://github.com/QC-LY/Prompt-Tuning-For-Sentiment-Classification)

We explore the differences in performance among Full Fineâ€tuning, Biasâ€term Fineâ€tuning and Promptâ€tuning in the process of building a preâ€ trainingâ€based sentiment classification model. To improve the performance of the prompt, we build a knowledgeâ€based verbalizer by acquiring knowledge from WordNet.

<div class='paper-box'><div class='paper-box-image'><div><img src='images/fundus.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Fundus Image Segmentation Base On Uâ€Net](https://github.com/QC-LY/Fundus-Image-Segmentation)

We used Uâ€net to classify the fundus image at the pixel level, setting the pixels in the vascular region to 1 and the pixels in the nonâ€vascular region to 0 to create a binary image that would segment the vascular region in the 2D fundus image.

# ğŸ“– Educations
- *2019.09 - 2023.06 (now)*, <br/>Bachelor in Artificial Intelligence, Northeastern University, Shenyang, China. 

# ğŸ– Honors and Awards
- *2020.09* Outstanding Students Scholarship, Northeastern University. 
- *2021.09* Outstanding Students Scholarship, Northeastern University. 
- *2023.09* Outstanding Students Scholarship, Northeastern University.
- *2021.11* The First Prize in China Undergraduate Mathematical Contest in Modeling, CSIAM.

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
